import os
from timeit import default_timer as timer
import random
import sys
import numpy as np
import torch
from lib import plotting
import pandas as pd

import helper
import pbr
import cb
import bayesopt
import arguments


sys.path.append("/home/ubuntu/autoscaler/vertical-pod-autoscaler/tools/agent")
sys.path.append("/home/ubuntu/autoscaler/vertical-pod-autoscaler/tools/lib")
sys.path.append("/home/ubuntu/autoscaler/vertical-pod-autoscaler/tools/environment")

import ddpg_vpa
from DummyContainerizedEnvCont import DummyContainerEnvCont
import ContainerizedEnv


def postprocess(parameters_meta, action):
        """
        The config generated by the algorithm is converted to the format that is expected by the interfaces of the next
        stage. For example, for categorical values, we are just tuning the indices of the list of allowed values. That
        index has to be convered to the actual categorical value.
        """
        def check_range_and_adjust(value, lower_limit, upper_limit):
            if value < lower_limit:
                value = lower_limit
            if value > upper_limit:
                value = upper_limit
            return value

        configs = []
        index = 0
        for _, row in parameters_meta.iterrows():
            if row["type"] == "categorical":  # categorical
                # for categorical, the value being tuned is the rounded index in the list of allowed values
                rounded_index = round(action[index])
                rounded_index = check_range_and_adjust(
                    rounded_index, 0, len(row["categorical_values"]))
                configs.append(
                    row["categorical_values"][rounded_index])
            else:
                value = action[index] 
                if row["type"] == "discrete":  # discrete
                    value = round(value)
                value = check_range_and_adjust( value
                    , row["lower_limit"], row["upper_limit"])
                configs.append(value)
            index += 1
        return configs

NUM_STATE_FEATURES=2
def ddpg_helper(parameters_file):
        # env_name = ['Pendulum-v1']
    env_index = -1
    # env = gym.make(env_name[env_index])
    # env_evaluate = gym.make(env_name[env_index])  # When evaluating the policy, we need to rebuild an environment
    number = 1
    env = ContainerizedEnv.ContainerEnv(num_features=NUM_STATE_FEATURES, parameters_file=parameters_file, args=args)
    env_evaluate = ContainerizedEnv.ContainerEnv(num_features=NUM_STATE_FEATURES,  parameters_file=parameters_file, args=args)
    # Set random seed
    seed = 0
    env.seed(seed)
    env.action_space.seed(seed)
    env_evaluate.seed(seed)
    env_evaluate.action_space.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    parameters_meta = pd.read_csv(parameters_file)
    upper_limits = parameters_meta["upper_limit"].values
    lower_limits = parameters_meta["lower_limit"].values 
    print(upper_limits)
    print(lower_limits)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])
    max_episode_steps = env._max_episode_steps  # Maximum number of steps per episode
    print("env={}".format("Containers"))
    print("state_dim={}".format(state_dim))
    print("action_dim={}".format(action_dim))
    print("max_action={}".format(max_action))
    print("max_episode_steps={}".format(max_episode_steps))

    agent = ddpg_vpa.DDPG(state_dim, action_dim, upper_limits, lower_limits)
    replay_buffer = ddpg_vpa.ReplayBuffer(state_dim, action_dim)
    # Build a tensorboard
    writer = ddpg_vpa.SummaryWriter(log_dir='./tools/agent/runs/DDPG/DDPG_env_{}_number_{}_seed_{}'.format("Containers", number, seed))

    epsilon = 0.1
    epsilon_decay = 0.9
    noise_std = lambda epsilon, max_action : epsilon * max_action  # the std of Gaussian noise for exploration
    # max_train_steps = 600  # Maximum number of training steps
    # random_steps = 200  # Take the random actions in the beginning for the better exploration
    # update_freq = 50  # Take 50 steps,then update the networks 50 times
    # evaluate_freq = 5   
    max_train_steps = 50  # Maximum number of training steps
    random_steps = 25  # Take the random actions in the beginning for the better exploration
    update_freq = 5  # Take 50 steps,then update the networks 50 times
    # evaluate_freq = 1  # Evaluate the policy every 'evaluate_freq' steps
    # evaluate_num = 0  # Record the number of evaluations
    # evaluate_rewards = []  # Record the rewards during the evaluating
    total_steps = 0  # Record the total steps during the training
    # Keeps track of useful statistics
    stats = ddpg_vpa.EpisodeStats(
        episode_lengths=np.zeros(100),
        episode_rewards=np.zeros(100))   
    ep = 0
    while total_steps < max_train_steps:
        s = env.reset()
        episode_steps = 0
        done = False
        
        while not done:
            episode_steps += 1
            if total_steps < random_steps:  # Take the random actions in the beginning for the better exploration
                a = env.action_space.sample()
            else:
                # Add Gaussian noise to actions for exploration
                epsilon = epsilon*(epsilon_decay**(total_steps/100))
                a = agent.choose_action(s)
                print(a)
                a = (a + np.random.normal(0, epsilon * upper_limits, size=action_dim)).clip(lower_limits, upper_limits)
                print(postprocess(parameters_meta, a))
            s_, r, done, _ = env.step(a)
            #print("Current step: ", env.current_step)
            # scales reward to converge
            # r = ddpg_vpa.reward_adapter(r, env_index)   # Adjust rewards for better performance
            # When dead or win or reaching the max_episode_steps, done will be Ture, we need to distinguish them;
            # dw means dead or win,there is no next state s';
            # but when reaching the max_episode_steps,there is a next state s' actually.
            # stats.episode_rewards[ep] += r
            # stats.episode_lengths[ep] = max_episode_steps
            if done and episode_steps != max_episode_steps:
                dw = True
            else:
                dw = False
            replay_buffer.store(s, a, r, s_, dw)  # Store the transition
            s = s_
            ep +=1
            # Take 50 steps,then update the networks 50 times
            if total_steps >= random_steps and total_steps % update_freq == 0:
                for _ in range(update_freq):
                    agent.learn(replay_buffer)
                    
            stats.episode_rewards[total_steps] += r
            stats.episode_lengths[total_steps] = max_episode_steps
            # Evaluate the policy every 'evaluate_freq' steps
            total_steps += 1
    fig1,fig2,fig3 = plotting.plot_episode_stats(stats, smoothing_window=50)
    fig1.savefig("vpa_ddpgfig1.png")
    fig2.savefig("vpa_ddpgfig2-10.png")
    fig3.savefig("vpa_ddpgfig3.png")
    

def run_algorithm(args):
    total_time = 0
    current_iteration = -1
    while(not approach_instance.stop_config_generation("dummy")):
        start = timer()
        config = approach_instance.next_config()
        print("config returned from approach_instance.next_config()")
        print(config)
        total_time += timer() - start
        current_iteration += 1
        objective_function_value = helper.get_reward(args, experiment_version_folder, current_iteration, config)
        #objective_function_value = random.randint(80,100)
        approach_instance.analysis(objective_function_value)
    print("Execution per iteration %f" % (total_time/current_iteration))


if __name__ == "__main__":
    args = arguments.argument_parser()
    experiment_version_folder = os.path.join(args.results_folder , args.experiment_type + "-" + args.experiment_version)
    helper.create_folder_p(experiment_version_folder)
    #TODO: copy all files to the expriment_version_folder to get back the exact results.
    for sequence_number in range(args.start_sequence, args.sequence_count+1):
        if args.approach == "PBR":
            approach_instance = pbr.PBR(args, experiment_version_folder, sequence_number, "configs/vpa_parameters.csv")
            run_algorithm(args)
        elif args.approach == "CB":
            approach_instance = cb.CBZO(args, experiment_version_folder, "configs/vpa_parameters.csv")
            run_algorithm(args)
        elif args.approach.startswith("bayesopt"):
            approach_instance = bayesopt.BayesianOptimization(args, experiment_version_folder)
            approach_instance.optimize()
        elif args.approach == "DDPG":
            print("ddpg")
            ddpg_helper("/home/ubuntu/autoscaler/vertical-pod-autoscaler/configs/vpa_parameters.csv")
